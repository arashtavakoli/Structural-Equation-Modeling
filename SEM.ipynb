{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is for preparing the data for SEM analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. labeling all the files for visualization purposes. This section can be skipped\n",
    "\n",
    "import pandas as pd \n",
    "import pandas as pd \n",
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "#preparing annotation file\n",
    "\n",
    "def get_sec(time_str):\n",
    "    \"\"\"Get Seconds from time.\"\"\"\n",
    "    #print(time_str)\n",
    "    h, m, s = time_str.split(':')\n",
    "    return int(h) * 3600 + int(m) * 60 + int(s)\n",
    "\n",
    "\n",
    "def time_transfer(dateandtime,video_name):\n",
    "    base_time = dt.datetime.strptime(video_name[:4]+\"-\"+video_name[4:6]+\"-\"+video_name[6:8]+\"T\"+video_name.split(\"_\")[1][:2]+\":\"+video_name.split(\"_\")[1][2:4]+\":\"+video_name.split(\"_\")[1][4:6]+\".000\",\"%Y-%m-%dT%H:%M:%S.%f\")-dt.timedelta(hours=4)#.microsecond\n",
    "    item_time = dt.datetime.strptime(dateandtime.split(\"+\")[0],\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    timestamp = ((item_time-base_time).seconds+(item_time-base_time).microseconds/1000000)-36000\n",
    "    return timestamp\n",
    "gt = pd.read_csv(\"H:/19/From Work/From Work.csv\")\n",
    "#gt = gt[gt[\"Video\"] == \"20200203_164117_NR.MP4\"] \n",
    "gt[\"time_start\"] = (gt[\"Video\"].str[:-7])\n",
    "gt[\"time_start\"] = pd.to_datetime(gt[\"time_start\"],format = '%Y%m%d_%H%M%S')\n",
    "gt[\"Timestamp Begin\"] = gt[\"Timestamp Begin\"].apply(get_sec)\n",
    "gt[\"Timestamp End\"] = gt[\"Timestamp End\"].apply(get_sec)\n",
    "gt[\"Timestamp Begin\"] = pd.to_timedelta(gt[\"Timestamp Begin\"],unit='s')\n",
    "gt[\"Timestamp End\"] = pd.to_timedelta(gt[\"Timestamp End\"],unit='s')\n",
    "gt[\"current_time_start\"] = gt[\"time_start\"] + gt[\"Timestamp Begin\"] \n",
    "gt[\"current_time_start\"] = gt[\"current_time_start\"].dt.tz_localize(None)\n",
    "gt[\"current_time_end\"] = gt[\"time_start\"] + gt[\"Timestamp End\"] \n",
    "gt[\"current_time_end\"] = gt[\"current_time_end\"].dt.tz_localize(None)\n",
    "\n",
    "gt_inside = gt[gt[\"Video\"].str.endswith(\"R.MP4\")]\n",
    "gt_outside = gt[gt[\"Video\"].str.endswith(\"F.MP4\")]\n",
    "gt_outside = gt_outside[gt_outside[\"Event\"] != \"driving in a city street\"]\n",
    "gt_inside = gt_inside[gt_inside[\"Event\"] != \"talking on phone without\"]\n",
    "\n",
    "\n",
    "target_file = acc.copy()\n",
    "target_file[\"Timestamp_new\"] = pd.to_datetime(target_file[\"Timestamp\"]) + pd.Timedelta(value=-5,unit='h')\n",
    "target_file[\"Timestamp_new\"] = target_file[\"Timestamp_new\"].dt.tz_localize(None)\n",
    "target_file[\"event_inside\"] =\"none\"\n",
    "target_file[\"event_outside\"] =\"none\"\n",
    "target_file[\"timestamp_inside\"] = \"none\"\n",
    "target_file[\"timestamp_outside\"] = \"none\"\n",
    "target_file[\"source_inside\"] = \"none\"\n",
    "target_file[\"source_outside\"] = \"none\"\n",
    "for index,row in gt_inside.iterrows():\n",
    "    target_file[\"event_inside\"][(target_file['Timestamp_new']>(row['current_time_start'])) \n",
    "                                & (target_file['Timestamp_new']<(row['current_time_end']))] = row[\"Event\"]\n",
    "    target_file[\"timestamp_inside\"][(target_file['Timestamp_new']>(row['current_time_start'])) \n",
    "                                & (target_file['Timestamp_new']<(row['current_time_end']))] = row[\"Timestamp Begin\"]\n",
    "    target_file[\"source_inside\"][(target_file['Timestamp_new']>(row['current_time_start'])) \n",
    "                                & (target_file['Timestamp_new']<(row['current_time_end']))] = row[\"Video\"]\n",
    "for index,row in gt_outside.iterrows():\n",
    "    target_file[\"event_outside\"][(target_file['Timestamp_new']>(row['current_time_start'])) \n",
    "                                & (target_file['Timestamp_new']<(row['current_time_end']))] = row[\"Event\"]\n",
    "    target_file[\"timestamp_outside\"][(target_file['Timestamp_new']>(row['current_time_start'])) \n",
    "                                & (target_file['Timestamp_new']<(row['current_time_end']))] = row[\"Timestamp Begin\"]\n",
    "    target_file[\"source_outside\"][(target_file['Timestamp_new']>(row['current_time_start'])) \n",
    "                                & (target_file['Timestamp_new']<(row['current_time_end']))] = row[\"Video\"]\n",
    "#target_file.to_csv(file[:-4]+\"_labeled\"+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.1 Reading all sources of wearable, vehicle, and gaze data\n",
    "\n",
    "#read car's data\n",
    "#reading acc,gyro, and linear acc data, put them together, resample at 100 hz, fill nans and make it a clean dataframe\n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "\n",
    "sensors = [\"GyroscopeDatum\",\"AccelerometerDatum\",\"LinearAccelerationDatum\"]\n",
    "df_ultimate = []\n",
    "\n",
    "for sensor in sensors:\n",
    "    df_temp = pd.read_csv(\"C:/Users/Arsalan/Desktop/CVILLE - DC Trips/both hands and car - DC - CVille/raw/car/\"+\"Smartwatch_\"+sensor+\".csv\",parse_dates=['Timestamp'])\n",
    "    #start_date = \"2020-08-21T23:30:59.154+0000\"\n",
    "    #end_date = \"2020-08-22T02:14:56.307+0000\"\n",
    "    start_date = \"2020-08-23T15:16:11.000+0000\"\n",
    "    end_date = \"2020-08-23T17:16:00.000+0000\"\n",
    "    mask = (df_temp['Timestamp'] > start_date) & (df_temp['Timestamp'] <= end_date)\n",
    "    df_temp = df_temp.loc[mask]\n",
    "    if sensor in [\"GyroscopeDatum\",\"AccelerometerDatum\",\"LinearAccelerationDatum\"]:\n",
    "        df_temp.rename(columns={'X': 'X'+sensor[:4], 'Y': 'Y'+sensor[:4],'Z':'Z'+sensor[:4]}, inplace=True)\n",
    "    df_ultimate.append(df_temp)\n",
    "\n",
    "    \n",
    "df_final = df_ultimate[0]\n",
    "for df in df_ultimate[1:]:\n",
    "    df = df.drop(columns=[\"participantid\"])\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    df_final = df_final.merge(df,on=['Timestamp',\n",
    "                                         'DeviceId'], how='outer')\n",
    "    \n",
    "    \n",
    "df_final = df_final.set_index(\"Timestamp\")\n",
    "df_final = df_final.groupby([\"DeviceId\"]).resample('100L').mean().ffill().bfill()\n",
    "df_final = df_final.reset_index()\n",
    "df_final[\"magGyro\"] = (df_final[\"XGyro\"]**2 + df_final[\"YGyro\"]**2 + df_final[\"ZGyro\"]**2)**0.5\n",
    "df_final[\"magAcce\"] = (df_final[\"XAcce\"]**2 + df_final[\"YAcce\"]**2 + df_final[\"ZAcce\"]**2)**0.5\n",
    "df_final[\"magLine\"] = (df_final[\"XLine\"]**2 + df_final[\"YLine\"]**2 + df_final[\"ZLine\"]**2)**0.5\n",
    "\n",
    "#keep a copy to not rerun things\n",
    "imu_data_car = df_final.copy()\n",
    "imu_data_car = imu_data_car.set_index(\"Timestamp\")\n",
    "imu_data_car = df_final.copy()\n",
    "imu_data_car = imu_data_car.set_index(\"Timestamp\")\n",
    "imu_data_car = imu_data_car[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.2 Reading all sources of wearable, vehicle, and gaze data\n",
    "#reading acc,gyro, and linear acc data, put them together, resample at 100 hz, fill nans and make it a clean dataframe\n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "\n",
    "sensors = [\"HeartRateDatum\",\"GyroscopeDatum\",\"AccelerometerDatum\",\"LinearAccelerationDatum\",\"Light\"]\n",
    "df_ultimate = []\n",
    "\n",
    "for sensor in sensors:\n",
    "    df_temp = pd.read_csv(\"C:/Users/Arsalan/Desktop/CVILLE - DC Trips/both hands and car - DC - CVille/raw/\"+\"Smartwatch_\"+sensor+\".csv\",parse_dates=['Timestamp'])\n",
    "#     start_date = \"2020-08-21T23:30:59.154+0000\"\n",
    "#     end_date = \"2020-08-22T02:14:56.307+0000\"\n",
    "    start_date = \"2020-08-23T15:16:11.000+0000\"\n",
    "    end_date = \"2020-08-23T17:16:00.000+0000\"\n",
    "    mask = (df_temp['Timestamp'] > start_date) & (df_temp['Timestamp'] <= end_date)\n",
    "    df_temp = df_temp.loc[mask]\n",
    "    if sensor in [\"GyroscopeDatum\",\"AccelerometerDatum\",\"LinearAccelerationDatum\"]:\n",
    "        df_temp.rename(columns={'X': 'X'+sensor[:4]+\"_driver\", 'Y': 'Y'+sensor[:4]+\"_driver\",'Z':'Z'+sensor[:4]+\"_driver\"}, inplace=True)\n",
    "    df_ultimate.append(df_temp)\n",
    "\n",
    "    \n",
    "df_final = df_ultimate[0]\n",
    "for df in df_ultimate[1:]:\n",
    "    df = df.drop(columns=[\"participantid\"])\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    df_final = df_final.merge(df,on=['Timestamp',\n",
    "                                         'DeviceId'], how='outer')\n",
    "    \n",
    "    \n",
    "df_final = df_final.set_index(\"Timestamp\")\n",
    "df_final = df_final.groupby([\"DeviceId\"]).resample('100L').mean().ffill().bfill()\n",
    "df_final = df_final.reset_index()\n",
    "df_final[\"magGyro_driver\"] = (df_final[\"XGyro_driver\"]**2 + df_final[\"YGyro_driver\"]**2 + df_final[\"ZGyro_driver\"]**2)**0.5\n",
    "df_final[\"magAcce_driver\"] = (df_final[\"XAcce_driver\"]**2 + df_final[\"YAcce_driver\"]**2 + df_final[\"ZAcce_driver\"]**2)**0.5\n",
    "df_final[\"magLine_driver\"] = (df_final[\"XLine_driver\"]**2 + df_final[\"YLine_driver\"]**2 + df_final[\"ZLine_driver\"]**2)**0.5\n",
    "\n",
    "#keep a copy to not rerun things\n",
    "hr_imu_data = df_final.copy()\n",
    "hr_imu_data = hr_imu_data.set_index(\"Timestamp\")\n",
    "hr_imu_data_left=hr_imu_data[hr_imu_data[\"DeviceId\"]==\"f60691a313420a4e\"]\n",
    "hr_imu_data_right=hr_imu_data[hr_imu_data[\"DeviceId\"]==\"39ca51c16b9ec429\"]\n",
    "hr_imu_data_left.columns = [str(col) + '_left' for col in hr_imu_data_left.columns]\n",
    "#imu_data_right = imu_data_right.set_index(\"Timestamp\")\n",
    "#imu_data_left = imu_data_left.set_index(\"Timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.3 Reading all sources of wearable, vehicle, and gaze data\n",
    "#reading gaze data\n",
    "\n",
    "cols = [ \" AU01_r\",\" AU02_r\",\" AU04_r\",\" AU05_r\",\" AU06_r\",\" AU07_r\",\" AU09_r\",\" AU10_r\",\" AU12_r\",\" AU14_r\",\" AU15_r\",\" AU17_r\",\" AU20_r\",\" AU23_r\",\" AU25_r\",\" AU26_r\",\" AU45_r\",\" AU01_c\",\" AU02_c\",\" AU04_c\",\" AU05_c\",\" AU06_c\",\" AU07_c\",\" AU09_c\",\" AU10_c\",\" AU12_c\",\" AU14_c\",\" AU15_c\",\" AU17_c\",\" AU20_c\",\" AU23_c\",\" AU25_c\",\" AU26_c\",\" AU28_c\", \" AU45_c\"\n",
    ",\" timestamp\",\" confidence\",\" success\",\" gaze_angle_x\",\" gaze_angle_y\",\" p_scale\",\" p_rx\",\" p_ry\",\" p_rz\",\" p_tx\",\" p_ty\",\"video_name\",\"day\" ]\n",
    "gaze = pd.read_csv(\"C:/Users/Arsalan/Desktop/CVILLE - DC Trips/both hands and car - DC - CVille/raw/final_gaze.csv\",usecols=cols)\n",
    "\n",
    "gaze[\"time_start\"] = (gaze[\"video_name\"].str[:-7])\n",
    "gaze[\"time_start\"] = pd.to_datetime(gaze[\"time_start\"],format = '%Y%m%d_%H%M%S')\n",
    "gaze[\" timestamp\"] = pd.to_timedelta(gaze[\" timestamp\"],unit = 's')\n",
    "gaze[\"Timestamp\"] = gaze[\"time_start\"] + gaze[\" timestamp\"]\n",
    "gaze[\"mag_gaze\"] = (gaze[\" gaze_angle_x\"]**2 + gaze[\" gaze_angle_y\"]**2)**0.5\n",
    "gaze[\"Timestamp\"] = gaze[\"Timestamp\"] + pd.Timedelta(value=+4,unit='h')\n",
    "# start_date = pd.Timestamp(\"2020-08-21T23:30:59.154+0000\",tz=None)\n",
    "# end_date = pd.Timestamp(\"2020-08-22T02:14:56.307+0000\",tz=None)\n",
    "start_date = pd.Timestamp(\"2020-08-23T15:16:11.000+0000\",tz=None)\n",
    "end_date = pd.Timestamp(\"2020-08-23T17:16:00.000+0000\",tz=None)\n",
    "start_date = start_date.tz_localize(None)\n",
    "end_date = end_date.tz_localize(None)\n",
    "gaze[\"Timestamp\"] = pd.to_datetime(gaze[\"Timestamp\"])  \n",
    "gaze[\"Timestamp\"] = gaze[\"Timestamp\"].dt.tz_localize(None)\n",
    "mask = (gaze[\"Timestamp\"] > start_date) & (gaze[\"Timestamp\"] <= end_date)\n",
    "gaze = gaze.loc[mask]\n",
    "gaze = gaze.set_index(\"Timestamp\")\n",
    "gaze = gaze[gaze[\" success\"] == 1]\n",
    "gaze = gaze[gaze[\" confidence\"]>0.85]\n",
    "gaze = gaze.resample('100L').mean().bfill().ffill()\n",
    "gaze = gaze.drop(columns=['day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.4 getting all the outside environment data\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "cols = [\"person\",\"bicycle\",\"car\",\"motorcycle\",\"bus\",\"truck\",\"frame\",\"follow_dist\",\"follow_obj\",\"lead_pres\",\"video_name\",\"day\" ]\n",
    "mask_rcnn = pd.read_csv(\"D:/Google Drive/UVA_PhD/Projects/Wearable Projects/Codes/Wearable_Event_detection/csv files/final_mask_rcnn.csv\",usecols=cols)\n",
    "\n",
    "mask_rcnn[\"time_start\"] = (mask_rcnn[\"video_name\"].str[:-16])\n",
    "mask_rcnn[\"time_start\"] = pd.to_datetime(mask_rcnn[\"time_start\"],format = '%Y%m%d_%H%M%S')\n",
    "mask_rcnn[\" timestamp\"] = mask_rcnn[\"frame\"]*0.0333\n",
    "mask_rcnn[\" timestamp\"] = pd.to_timedelta(mask_rcnn[\" timestamp\"],unit = 's')\n",
    "mask_rcnn[\"Timestamp\"] = mask_rcnn[\"time_start\"] + mask_rcnn[\" timestamp\"]\n",
    "mask_rcnn[\"Timestamp\"] = mask_rcnn[\"Timestamp\"] + pd.Timedelta(value=+4,unit='h')\n",
    "# start_date = pd.Timestamp(\"2020-08-21T23:30:59.154+0000\",tz=None)\n",
    "# end_date = pd.Timestamp(\"2020-08-22T02:14:56.307+0000\",tz=None)\n",
    "start_date = pd.Timestamp(\"2020-08-23T15:16:11.000+0000\",tz=None)\n",
    "end_date = pd.Timestamp(\"2020-08-23T17:16:00.000+0000\",tz=None)\n",
    "start_date = start_date.tz_localize(None)\n",
    "end_date = end_date.tz_localize(None)\n",
    "mask_rcnn[\"Timestamp\"] = pd.to_datetime(mask_rcnn[\"Timestamp\"])  \n",
    "mask_rcnn[\"Timestamp\"] = mask_rcnn[\"Timestamp\"].dt.tz_localize(None)\n",
    "mask = (mask_rcnn[\"Timestamp\"] > start_date) & (mask_rcnn[\"Timestamp\"] <= end_date)\n",
    "mask_rcnn = mask_rcnn.loc[mask]\n",
    "mask_rcnn = mask_rcnn.set_index(\"Timestamp\")\n",
    "mask_rcnn[\"lead_pres\"] = mask_rcnn[\"lead_pres\"].fillna(0)\n",
    "mask_rcnn[\"follow_obj\"] = mask_rcnn[\"follow_obj\"].fillna(15)\n",
    "mask_rcnn[\"follow_dist\"] = mask_rcnn[\"follow_dist\"].fillna(5000)\n",
    "mask_rcnn = mask_rcnn.resample('100L').bfill().ffill()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. put them all together\n",
    "\n",
    "hr_imu_data_right.index = hr_imu_data_right.index.tz_localize(None)\n",
    "hr_imu_data_left.index = hr_imu_data_left.index.tz_localize(None)\n",
    "imu_data_car.index = imu_data_car.index.tz_localize(None)\n",
    "total_data = pd.concat([mask_rcnn,gaze,hr_imu_data_right,hr_imu_data_left,imu_data_car],axis=1)\n",
    "\n",
    "#4. combining them with change points - copying bcp groups from previous steps\n",
    "total_data = total_data[:-1]\n",
    "#total_data[\"bcp_groups\"] = list_groups[\"new_group\"].values\n",
    "#total_data = total_data.dropna()\n",
    "total_data[\"gaze_std_x\"]=total_data[\" gaze_angle_x\"].rolling(10,min_periods=0).std()\n",
    "total_data[\"gaze_std_x\"] = total_data[\"gaze_std_x\"].bfill()\n",
    "total_data[\"gaze_std_y\"]=total_data[\" gaze_angle_y\"].rolling(10,min_periods=0).std()\n",
    "total_data[\"gaze_std_y\"] = total_data[\"gaze_std_y\"].bfill()\n",
    "total_data = total_data.bfill().ffill()\n",
    "#total_data.to_csv(\"data_coming_back_100L.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#4. exploring different discreetizations\n",
    "\n",
    "import plotly.express as px\n",
    "from sklearn import mixture\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "data_all = total_data.copy()\n",
    "#data = data_all.drop([' confidence',' success','Timestamp','DeviceId','DeviceId.1','video_name'],axis=1)\n",
    "for col in data_all.columns:\n",
    "    if col in ['gaze_std','follow_dist', ' gaze_angle_x', ' gaze_angle_y',\n",
    "       ' p_scale', ' p_rx', ' p_ry', ' p_rz', ' p_tx', ' p_ty', 'mag_gaze',\n",
    "       'HR', 'XGyro_driver', 'YGyro_driver', 'ZGyro_driver', 'XAcce_driver',\n",
    "       'YAcce_driver', 'ZAcce_driver', 'XLine_driver', 'YLine_driver',\n",
    "       'ZLine_driver', 'Light', 'magGyro_driver', 'magAcce_driver',\n",
    "       'magLine_driver', 'XGyro', 'YGyro', 'ZGyro', 'XAcce', 'YAcce', 'ZAcce',\n",
    "       'XLine', 'YLine', 'ZLine', 'magGyro', 'magAcce', 'magLine']:\n",
    "        if col==\"mag_gaze\":\n",
    "            print(col)\n",
    "            temp = data_all[col].copy()\n",
    "            X = temp.values\n",
    "            X = X.reshape(-1,1)\n",
    "            labels = (mixture.GaussianMixture(n_components=2).fit(X)).predict(X)\n",
    "            data_all[(col+str(\"_label\"))] = labels\n",
    "data_all = data_all.reset_index()\n",
    "fig = px.scatter(data_all, x=\"Timestamp\", y=\"mag_gaze\",color= \"mag_gaze_label\",labels={\n",
    "                     \"Timestamp\": \"<b>Time\",\n",
    "                     \"HR\": \"<b>HR (bpm)\",\n",
    "                 })\n",
    "\n",
    "fig.update_layout(    \n",
    "        {   \n",
    "            'yaxis':{'dtick': 10,'title':\"<b>HR (bpm)\",'linecolor':'black','linewidth':10},\n",
    "            'xaxis':{'title':\"<b>Time\",'linecolor':'black','linewidth':10},\n",
    "            'height':600,\n",
    "            'width':1200,\n",
    "            'font':dict(size=24),\n",
    "            'paper_bgcolor':'rgba(0,0,0,0)',\n",
    "            'plot_bgcolor':'rgba(0,0,0,0)'\n",
    "        }\n",
    "    )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. reading all the data that is saved from the previous sections not necessary\n",
    "# you can skip this part and go to #7\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import mixture\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "data_all = pd.read_csv(\"D:/Google Drive/UVA_PhD/Projects/Structural Equation Modeling/data.csv\")\n",
    "data = data_all.drop(['Timestamp','DeviceId','DeviceId.1','video_name'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. finding the best discreetization cluster number for gmm algorithmically not necessary\n",
    "# you can skip this part and go to #7\n",
    "\n",
    "for col in data.columns:\n",
    "    print(col)\n",
    "    temp = data[col]\n",
    "    X = temp.values\n",
    "    X = X.reshape(-1,1)\n",
    "    n_components = np.arange(1, 15)\n",
    "    models = [mixture.GaussianMixture(n, random_state=0).fit(X)\n",
    "              for n in n_components]\n",
    "    plt.plot(n_components, [m.bic(X) for m in models], label='BIC')\n",
    "    plt.plot(n_components, [m.aic(X) for m in models], label='AIC')\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel('n_components');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. putting the clusters manually as a dictionray and clustering them \n",
    "\n",
    "from sklearn import mixture\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "#data_all = total_data.copy()\n",
    "components_modal = {' gaze_angle_x':2, ' gaze_angle_y':2,'mag_gaze':2}\n",
    "\n",
    "for col in data_all.columns:\n",
    "    if col in [' gaze_angle_x',' gaze_angle_y','mag_gaze']:\n",
    "        print(col)\n",
    "        temp = data_all[col]\n",
    "        X = temp.values\n",
    "        X = X.reshape(-1,1)\n",
    "        labels = (mixture.GaussianMixture(n_components=components_modal[col]).fit(X)).predict(X)\n",
    "        data_all[(col+str(\"_label\"))] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. Adding road type info \n",
    "\n",
    "start_date = pd.Timestamp(\"2020-08-23T15:25:22.000+0000\",tz=None)\n",
    "end_date = pd.Timestamp(\"2020-08-23T17:16:00.000+0000\",tz=None)\n",
    "start_date = start_date.tz_localize(None)\n",
    "end_date = end_date.tz_localize(None)\n",
    "data_all.index = pd.to_datetime(data_all.index)\n",
    "data_all[\"road_type\"] = None\n",
    "data_all.loc[data_all.index<start_date,'road_type']=0\n",
    "data_all.loc[data_all.index>=start_date,'road_type']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. Adding time factor\n",
    "\n",
    "time_1 = pd.Timestamp(\"2020-08-23T15:25:21.000+0000\",tz=None)\n",
    "time_2 = pd.Timestamp(\"2020-08-23T16:21:21.000+0000\",tz=None)\n",
    "time_3 = pd.Timestamp(\"2020-08-23T17:21:21.000+0000\",tz=None)\n",
    "time_1= time_1.tz_localize(None)\n",
    "time_2= time_2.tz_localize(None)\n",
    "time_3= time_3.tz_localize(None)\n",
    "\n",
    "data_all[\"time_factor\"] = None\n",
    "data_all.loc[data_all.index<time_2,'time_factor']=0\n",
    "data_all.loc[data_all.index>=time_2,'time_factor']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all.to_csv(\"data_everything.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10.1 Same analysis for 03072021 trip - watch\n",
    "\n",
    "#reading acc,gyro, and linear acc data, put them together, resample at 100 hz, fill nans and make it a clean dataframe\n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "from scipy.signal import find_peaks,detrend\n",
    "\n",
    "sensors = [\"HeartRateDatum\",\"GyroscopeDatum\",\"AccelerometerDatum\",\"LinearAccelerationDatum\",\"PPGDatum\"]\n",
    "df_ultimate = []\n",
    "\n",
    "for sensor in sensors:\n",
    "    df_temp = pd.read_csv(\"C:/Users/Arsalan/Desktop/CVILLE - DC Trips/03072021 DC- CVille/\"+\"Smartwatch_\"+sensor+\".csv\",parse_dates=['Timestamp'])\n",
    "    start_date = \"2021-03-05T20:31:21.000+0000\"\n",
    "    end_date = \"2021-03-05T23:27:00.000+0000\"\n",
    "#     start_date = \"2020-08-23T15:16:11.000+0000\"\n",
    "#     end_date = \"2020-08-23T17:16:00.000+0000\"\n",
    "    mask = (df_temp['Timestamp'] > start_date) & (df_temp['Timestamp'] <= end_date)\n",
    "    df_temp = df_temp.loc[mask]\n",
    "    if sensor in [\"GyroscopeDatum\",\"AccelerometerDatum\",\"LinearAccelerationDatum\"]:\n",
    "        df_temp.rename(columns={'X': 'X'+sensor[:4]+\"_driver\", 'Y': 'Y'+sensor[:4]+\"_driver\",'Z':'Z'+sensor[:4]+\"_driver\"}, inplace=True)\n",
    "    df_ultimate.append(df_temp)\n",
    "\n",
    "    \n",
    "df_final = df_ultimate[0]\n",
    "for df in df_ultimate[1:]:\n",
    "    df = df.drop(columns=[\"participantid\"])\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    df_final = df_final.merge(df,on=['Timestamp',\n",
    "                                         'DeviceId'], how='outer')\n",
    "    \n",
    "    \n",
    "df_final = df_final.set_index(\"Timestamp\")\n",
    "df_final = df_final.groupby([\"DeviceId\"]).resample('10L').mean().ffill().bfill()\n",
    "df_final = df_final.reset_index()\n",
    "df_final[\"magGyro_driver\"] = (df_final[\"XGyro_driver\"]**2 + df_final[\"YGyro_driver\"]**2 + df_final[\"ZGyro_driver\"]**2)**0.5\n",
    "df_final[\"magAcce_driver\"] = (df_final[\"XAcce_driver\"]**2 + df_final[\"YAcce_driver\"]**2 + df_final[\"ZAcce_driver\"]**2)**0.5\n",
    "df_final[\"magLine_driver\"] = (df_final[\"XLine_driver\"]**2 + df_final[\"YLine_driver\"]**2 + df_final[\"ZLine_driver\"]**2)**0.5\n",
    "df_final[\"magGyro_driver\"] = (detrend(df_final[\"magGyro_driver\"].values))**2\n",
    "df_final[\"magAcce_driver\"] = (detrend(df_final[\"magAcce_driver\"].values))**2\n",
    "df_final[\"magLine_driver\"] = (detrend(df_final[\"magLine_driver\"].values))**2\n",
    "\n",
    "#keep a copy to not rerun things\n",
    "hr_imu_data = df_final.copy()\n",
    "hr_imu_data = hr_imu_data.set_index(\"Timestamp\")\n",
    "#imu_data_right = imu_data_right.set_index(\"Timestamp\")\n",
    "#imu_data_left = imu_data_left.set_index(\"Timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10.2 Same analysis for 03072021 trip - gaze\n",
    "#check for time difference \n",
    "\n",
    "cols = [ \" AU01_r\",\" AU02_r\",\" AU04_r\",\" AU05_r\",\" AU06_r\",\" AU07_r\",\" AU09_r\",\" AU10_r\",\" AU12_r\",\" AU14_r\",\" AU15_r\",\" AU17_r\",\" AU20_r\",\" AU23_r\",\" AU25_r\",\" AU26_r\",\" AU45_r\",\" AU01_c\",\" AU02_c\",\" AU04_c\",\" AU05_c\",\" AU06_c\",\" AU07_c\",\" AU09_c\",\" AU10_c\",\" AU12_c\",\" AU14_c\",\" AU15_c\",\" AU17_c\",\" AU20_c\",\" AU23_c\",\" AU25_c\",\" AU26_c\",\" AU28_c\", \" AU45_c\"\n",
    ",\" timestamp\",\" confidence\",\" success\",\" gaze_angle_x\",\" gaze_angle_y\",\" p_scale\",\" p_rx\",\" p_ry\",\" p_rz\",\" p_tx\",\" p_ty\",\"video_name\",\"day\" ]\n",
    "gaze = pd.read_csv(\"I:/Phase 1/9/Video/03072021/OpenFace/final_gaze.csv\",usecols=cols)\n",
    "\n",
    "gaze[\"time_start\"] = (gaze[\"video_name\"].str[:-7])\n",
    "gaze[\"time_start\"] = pd.to_datetime(gaze[\"time_start\"],format = '%Y%m%d_%H%M%S')\n",
    "gaze[\" timestamp\"] = pd.to_timedelta(gaze[\" timestamp\"],unit = 's')\n",
    "gaze[\"Timestamp\"] = gaze[\"time_start\"] + gaze[\" timestamp\"]\n",
    "gaze[\"mag_gaze\"] = (gaze[\" gaze_angle_x\"]**2 + gaze[\" gaze_angle_y\"]**2)**0.5\n",
    "gaze[\"Timestamp\"] = gaze[\"Timestamp\"] + pd.Timedelta(value=+5,unit='h')\n",
    "start_date = pd.Timestamp(\"2021-03-05T20:31:21.000+0000\",tz=None)\n",
    "end_date = pd.Timestamp(\"2021-03-05T23:27:00.000+0000\",tz=None)\n",
    "#start_date = pd.Timestamp(\"2020-08-23T15:16:11.000+0000\",tz=None)\n",
    "#end_date = pd.Timestamp(\"2020-08-23T17:16:00.000+0000\",tz=None)\n",
    "start_date = start_date.tz_localize(None)\n",
    "end_date = end_date.tz_localize(None)\n",
    "gaze[\"Timestamp\"] = pd.to_datetime(gaze[\"Timestamp\"])  \n",
    "gaze[\"Timestamp\"] = gaze[\"Timestamp\"].dt.tz_localize(None)\n",
    "mask = (gaze[\"Timestamp\"] > start_date) & (gaze[\"Timestamp\"] <= end_date)\n",
    "gaze = gaze.loc[mask]\n",
    "gaze = gaze.set_index(\"Timestamp\")\n",
    "gaze = gaze[gaze[\" success\"] == 1]\n",
    "gaze = gaze[gaze[\" confidence\"]>0.85]\n",
    "gaze = gaze.resample('10L').mean().bfill().ffill()\n",
    "gaze = gaze.drop(columns=['day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10.3 Same analysis for 03072021 trip - mask rcnn\n",
    "#check for time difference \n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "cols = [\"person\",\"bicycle\",\"car\",\"motorcycle\",\"bus\",\"truck\",\"frame\",\"follow_dist\",\"follow_obj\",\"lead_pres\",\"video_name\",\"day\",\"traffic light\" ]\n",
    "mask_rcnn = pd.read_csv(\"D:/Google Drive/UVA_PhD/Projects/Structural Equation Modeling/csv files/final_mask_rcnn.csv\",usecols=cols)\n",
    "\n",
    "mask_rcnn[\"time_start\"] = (mask_rcnn[\"video_name\"].str[:-16])\n",
    "mask_rcnn[\"time_start\"] = pd.to_datetime(mask_rcnn[\"time_start\"],format = '%Y%m%d_%H%M%S')\n",
    "mask_rcnn[\" timestamp\"] = mask_rcnn[\"frame\"]*0.0333\n",
    "mask_rcnn[\" timestamp\"] = pd.to_timedelta(mask_rcnn[\" timestamp\"],unit = 's')\n",
    "mask_rcnn[\"Timestamp\"] = mask_rcnn[\"time_start\"] + mask_rcnn[\" timestamp\"]\n",
    "mask_rcnn[\"Timestamp\"] = mask_rcnn[\"Timestamp\"] + pd.Timedelta(value=+5,unit='h')\n",
    "start_date = pd.Timestamp(\"2021-03-05T20:31:21.000+0000\",tz=None)\n",
    "end_date = pd.Timestamp(\"2021-03-05T23:27:00.000+0000\",tz=None)\n",
    "#start_date = pd.Timestamp(\"2020-08-23T15:16:11.000+0000\",tz=None)\n",
    "#end_date = pd.Timestamp(\"2020-08-23T17:16:00.000+0000\",tz=None)\n",
    "start_date = start_date.tz_localize(None)\n",
    "end_date = end_date.tz_localize(None)\n",
    "mask_rcnn[\"Timestamp\"] = pd.to_datetime(mask_rcnn[\"Timestamp\"])  \n",
    "mask_rcnn[\"Timestamp\"] = mask_rcnn[\"Timestamp\"].dt.tz_localize(None)\n",
    "mask = (mask_rcnn[\"Timestamp\"] > start_date) & (mask_rcnn[\"Timestamp\"] <= end_date)\n",
    "mask_rcnn = mask_rcnn.loc[mask]\n",
    "mask_rcnn = mask_rcnn.set_index(\"Timestamp\")\n",
    "mask_rcnn[\"lead_pres\"] = mask_rcnn[\"lead_pres\"].fillna(0)\n",
    "mask_rcnn[\"follow_obj\"] = mask_rcnn[\"follow_obj\"].fillna(15)\n",
    "mask_rcnn[\"follow_dist\"] = mask_rcnn[\"follow_dist\"].fillna(mask_rcnn[\"follow_dist\"].max(), downcast='infer')\n",
    "mask_rcnn[\"all_vehicles\"] = mask_rcnn[\"car\"] + mask_rcnn[\"truck\"]\n",
    "mask_rcnn = mask_rcnn.resample('10L').bfill().ffill()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10.4 Same analysis for 03072021 trip put them all together\n",
    "\n",
    "hr_imu_data.index = hr_imu_data.index.tz_localize(None)\n",
    "total_data = pd.concat([mask_rcnn,gaze,hr_imu_data],axis=1)\n",
    "\n",
    "#4. combining them with change points - copying bcp groups from previous steps\n",
    "total_data = total_data[:-1]\n",
    "total_data = total_data.bfill().ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10.5 combining with hav data - you can also get the total_data and run hav on it\n",
    "\n",
    "features_needed = ['all_vehicles','traffic light','follow_dist',' gaze_angle_x',' gaze_angle_y',\n",
    "                  ' AU01_c', ' AU02_c', ' AU04_c', ' AU05_c', ' AU06_c', ' AU07_c',\n",
    "                   ' AU09_c', ' AU10_c', ' AU12_c', ' AU14_c', ' AU15_c', ' AU17_c',\n",
    "                   ' AU20_c', ' AU23_c', ' AU25_c', ' AU26_c', ' AU28_c', ' AU45_c',\n",
    "                  'PPG1','PPG3', 'HR']\n",
    "total_data_cleaned = total_data[features_needed]\n",
    "\n",
    "#if you need to save it run this \n",
    "#total_data_cleaned.to_csv(\"03072021_going_raw.csv\")\n",
    "\n",
    "total_data_cleaned = total_data_cleaned.reset_index()\n",
    "hav_inc = pd.read_csv(\"csv files/hav_inc_cleaned_03072021.csv\")\n",
    "feat_list = [\"PPG1 _hav\",\"PPG3 _hav\",\"gaze_angle_x _hav\",\"gaze_angle_y _hav\",\"follow_dist _hav\"]\n",
    "hav_inc = hav_inc[feat_list]\n",
    "total = pd.concat([hav_inc,total_data_cleaned],axis=1)\n",
    "total = total.dropna()\n",
    "total = total.rename(columns={\"PPG1 _hav\":\"PPG1_hav\",\"PPG3 _hav\":\"PPG3_hav\",\n",
    "                              \"gaze_angle_x _hav\":\"gaze_angle_x_hav\",\n",
    "                              \"gaze_angle_y _hav\":\"gaze_angle_y_hav\"\n",
    "                             ,\"traffic light\":\"traffic_light\"})\n",
    "\n",
    "total[\"PPG1_hav\"] = total[\"PPG1_hav\"]#**2\n",
    "total[\"PPG3_hav\"] = total[\"PPG3_hav\"]#**2\n",
    "total[\"gaze_angle_x_hav\"] = total[\"gaze_angle_x_hav\"]**2\n",
    "total[\"gaze_angle_y_hav\"] = total[\"gaze_angle_y_hav\"]**2\n",
    "\n",
    "\n",
    "total = total.set_index(\"Timestamp\")\n",
    "#total = total.resample(\"1000L\").mean()\n",
    "#total.to_csv(\"1s_resampled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11.0 Smoothing - exploration - high frequency\n",
    "\n",
    "total[\"PPG1_hav_smooth\"] = signal.savgol_filter(total[\"PPG1_hav\"],499,2)\n",
    "total[\"PPG3_hav_smooth\"] = signal.savgol_filter(total[\"PPG3_hav\"],499,2)\n",
    "total[\"gaze_angle_x_hav_smooth\"] = signal.savgol_filter(total[\"gaze_angle_x_hav\"],499,2)\n",
    "total[\"car_smooth\"] = signal.savgol_filter(total[\"car\"],499,2)\n",
    "total[\"follow_dist_smooth\"] = signal.savgol_filter(total[\"follow_dist\"],499,2)\n",
    "total[\"traffic_light_smooth\"] = signal.savgol_filter(total[\"traffic_light\"],499,2)\n",
    "total = total.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11.1 Smoothing - exploration\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "data_for_resample = pd.read_csv(\"1s_resampled_change_p_higher_p0.csv\")\n",
    "data_for_resample = data_for_resample.set_index(\"Timestamp\")\n",
    "\n",
    "#data_for_resample = total_resampled.resample('1S').mean()\n",
    "data_for_resample[\"PPG1_hav_smooth\"] = signal.savgol_filter(data_for_resample[\"PPG1_hav\"],501,3)\n",
    "data_for_resample[\"PPG3_hav_smooth\"] = signal.savgol_filter(data_for_resample[\"PPG3_hav\"],501,3)\n",
    "data_for_resample[\"gaze_angle_x_hav_smooth\"] = signal.savgol_filter(data_for_resample[\"gaze_angle_x_hav\"],501,3)\n",
    "data_for_resample[\"car_smooth\"] = signal.savgol_filter(data_for_resample[\"all_vehicles\"],2599,6)\n",
    "data_for_resample[\"follow_dist_smooth\"] = signal.savgol_filter(data_for_resample[\"follow_dist\"],501,3)\n",
    "data_for_resample[\"traffic_light_smooth\"] = signal.savgol_filter(data_for_resample[\"traffic_light\"],501,3)\n",
    "data_for_resample[\"AU06_c_smooth\"] = signal.savgol_filter(data_for_resample[\"AU06_c\"],501,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11.2 Smoothing - real\n",
    "from scipy import signal\n",
    "\n",
    "data_for_resample = pd.read_csv(\"1s_resampled_change_p_higher_p0.csv\")\n",
    "data_for_resample = data_for_resample.set_index(\"Timestamp\")\n",
    "features = {'all_vehicles':121,'follow_dist':501,'traffic_light':501,'gaze_angle_x_hav':501,\n",
    "       'gaze_angle_y_hav':501,'AU01_c':501, 'AU02_c':501, \n",
    "            'AU04_c':501, 'AU05_c':501, 'AU06_c':501, 'AU07_c':501,\n",
    "       'AU09_c':501, 'AU10_c':501, 'AU12_c':501, 'AU14_c':501, 'AU15_c':501, 'AU17_c':501,\n",
    "       'AU20_c':501, 'AU23_c':501, 'AU25_c':501, 'AU26_c':501, 'AU28_c':501, 'AU45_c':501, 'HR _bcp_prob':501,\n",
    "           'PPG1_hav':501,'PPG3_hav':501}\n",
    "for key in features:\n",
    "    data_for_resample[str(key)+\"_smooth\"] = signal.savgol_filter(data_for_resample[key],features[key],2)\n",
    "data_for_resample = data_for_resample.reset_index()\n",
    "#data_for_resample.to_csv(\"all_data_1s_higher_p0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_peak = pd.read_csv(\"1s_resampled_change_p_higher_p0.csv\",parse_dates=[\"Timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking the rising change points\n",
    "\n",
    "data_for_peak[\"Timestamp\"] = data_for_peak[\"Timestamp\"].dt.tz_localize(None)\n",
    "data_for_peak[\"diff\"] = data_for_peak[\"HR_bcp_mean\"].diff()\n",
    "data_for_peak[\"diff\"] = data_for_peak[\"diff\"].shift(-1)\n",
    "data_for_peak[\"type_of_cp\"] = data_for_peak[\"diff\"].apply(lambda x: 1 if x>0.9 else 0)\n",
    "data_for_peak[\"rising_cp\"] = 0\n",
    "data_for_peak[\"rising_cp\"][(data_for_peak[\"HR_bcp_prob\"]!=0)&(data_for_peak[\"type_of_cp\"]==1)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resampel accelerometer data and combine\n",
    "\n",
    "hr_imu_data_res = hr_imu_data.resample(\"1S\").mean()\n",
    "hr_imu_data_res = hr_imu_data_res.reset_index()\n",
    "hr_imu_data_res[\"Timestamp\"] = hr_imu_data_res[\"Timestamp\"].dt.tz_localize(None)\n",
    "data_with_acc = pd.merge(hr_imu_data_res,data_for_peak,on=\"Timestamp\")\n",
    "data_for_peak = data_with_acc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "from scipy.signal import find_peaks\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features = {'all_vehicles_bcp_prob':0.1,'follow_dist_bcp_prob':0.5,'traffic_light':3,\n",
    "           'PPG1_hav':9.493063780879695e-07,'PPG3_hav':9.493065626152543e-07,\"HR_bcp_prob\":1,\"AU06_c\":0.1,\"AU12_c\":0.1\n",
    "           ,\"magGyro_driver\":0.2,\"magAcce_driver\":1,\"rising_cp\":1}\n",
    "features_dist = {'all_vehicles_bcp_prob':1,'follow_dist_bcp_prob':1,'traffic_light':20,\n",
    "           'PPG1_hav':1,'PPG3_hav':1,\"HR_bcp_prob\":600,\"AU06_c\":1,\"AU12_c\":1,\"magGyro_driver\":1,\"magAcce_driver\":1,\"rising_cp\":600}\n",
    "\n",
    "for feat in features:\n",
    "    #plt.figure()\n",
    "    x = data_for_peak[feat]\n",
    "    peaks, _ = find_peaks(x, height=features[feat],distance=features_dist[feat])\n",
    "    #sns.distplot(peaks[1:]-peaks[:-1])\n",
    "    boolean_obs = np.zeros(x.shape[0])\n",
    "    boolean_obs[peaks] = 1\n",
    "    data_for_peak[feat+\"boolean\"] = boolean_obs\n",
    "    data_for_peak[feat+\"coin_prob\"] = data_for_peak[feat+\"boolean\"].rolling(window=240,min_periods=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invgauss_fit(bool_peaks, get='hr'):\n",
    "    peaks = np.where(bool_peaks==1)[0]\n",
    "    waiting_times = np.array(peaks[1:]-peaks[:-1])\n",
    "    frequencies = 1.0/waiting_times\n",
    "    mu = waiting_times.mean()\n",
    "    #sns.distplot(frequencies)\n",
    "    theta = 1.0/(((frequencies - 1.0/mu).sum())/frequencies.shape[0])\n",
    "    \n",
    "    mean_arrival_rate = 1.0/mu + 1.0/theta\n",
    "    std_arrival_rate = np.sqrt(1.0/(mu*theta) + 2.0/(theta*theta))\n",
    "    std_arrival_time = np.sqrt(mu**3/theta)\n",
    "    \n",
    "    if get == 'hr':\n",
    "        return mean_arrival_rate\n",
    "    elif get=='hrv':\n",
    "        return std_arrival_rate\n",
    "    \n",
    "def inv1(bool_peaks):\n",
    "    return invgauss_fit(bool_peaks, get='hr')\n",
    "\n",
    "def inv2(bool_peaks):\n",
    "    return invgauss_fit(bool_peaks, get='hrv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {'all_vehicles_bcp_prob':0.1,'follow_dist_bcp_prob':0.5,'traffic_light':1,\n",
    "           'PPG1_hav':9.493063780879695e-07,'PPG3_hav':9.493065626152543e-07,\"HR_bcp_prob\":0.1,\"AU06_c\":0.1,\"AU12_c\":0.1,\n",
    "            \"magGyro_driver\":0.2,\"magAcce_driver\":1}\n",
    "for feat in features:\n",
    "    data_for_peak[feat+\"inv\"] = data_for_peak[feat+\"boolean\"].rolling(window=240,min_periods=1).apply(inv1)\n",
    "\n",
    "data_for_peak[\"traffic_light_moving\"] = data_for_peak[\"traffic_lightboolean\"].rolling(600,min_periods=0).mean()\n",
    "data_for_peak = data_for_peak.bfill().ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data from PPG feature Extraction - See PPG Analysis folder notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nk PPG\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "PPG_features = pd.read_csv(\"All_features_PPG3.csv\",parse_dates=[\"Timestamp\"])\n",
    "PPG_features = PPG_features.set_index(\"Timestamp\")\n",
    "PPG_features = PPG_features.resample(\"1S\").mean()\n",
    "PPG_features = PPG_features.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heartpy PPG\n",
    "PPG_features = pd.read_csv(\"D:/Google Drive/UVA_PhD/Projects/PPG Analysis/PPG A 0307/final_heartpy_PPG.csv\",parse_dates=[\"Timestamp\"])\n",
    "mask_rmssd = np.where(np.isclose(PPG_features['RMSSD'], 7777))\n",
    "mask_breathing = np.where(np.isclose(PPG_features['breathing'], 7777))\n",
    "PPG_features[\"RMSSD\"].loc[mask_rmssd] = np.nan\n",
    "PPG_features[\"breathing\"].loc[mask_breathing] = np.nan\n",
    "PPG_features = PPG_features.set_index(\"Timestamp\")\n",
    "PPG_features = PPG_features.resample(\"1S\").mean()\n",
    "PPG_features = PPG_features.reset_index()\n",
    "PPG_features[\"Timestamp\"] = PPG_features[\"Timestamp\"].dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data from gaze feature Extraction - See gaze feature extraction folder notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gaze_ent = pd.read_csv(\"D:/Google Drive/UVA_PhD/Projects/gaze feature extraction/gaze_ent_in.csv\",parse_dates=[\"Timestamp\"])\n",
    "gaze_ent = gaze_ent.set_index([\"Timestamp\"])\n",
    "gaze_ent = gaze_ent[\"entropy\"]\n",
    "gaze_ent = gaze_ent.resample(\"1S\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze_ent = gaze_ent.reset_index()\n",
    "final_data = pd.merge(PPG_features,data_for_peak,on=\"Timestamp\")\n",
    "final_data = pd.merge(final_data,gaze_ent,on=\"Timestamp\")\n",
    "final_data = final_data.set_index(\"Timestamp\")\n",
    "final_data = final_data.reset_index()\n",
    "final_data[\"PPG3_hav_moving\"] = final_data[\"PPG3_hav\"].rolling(240,min_periods=0).mean()\n",
    "final_data[\"all_vehicles_moving\"] = final_data[\"all_vehicles_bcp_prob\"].rolling(240,min_periods=1).mean()\n",
    "final_data[\"magGyro_driver_moving\"] = final_data[\"magGyro_driverboolean\"].rolling(240,min_periods=1).mean()\n",
    "final_data[\"all_vehicles_moving\"] = final_data[\"all_vehicles\"].rolling(240,min_periods=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move here and read the file below for saving time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "final_data = pd.read_csv(\"state_space_model_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12. plotting the smooth\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import wasserstein_distance\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import scipy\n",
    "from scipy import signal\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "#\n",
    "data = final_data.copy()\n",
    "fig1 = make_subplots(rows=3, cols=1, \n",
    "                    shared_xaxes=True,vertical_spacing =0.05,specs=[[{\"secondary_y\": True}],[{\"secondary_y\": True}]\n",
    "                                                                   ,[{\"secondary_y\": True}]])\n",
    "fig1.add_trace(go.Scatter(x=data[\"Timestamp\"], y=data[\"HR_bcp_mean\"],name= \"entropy\",yaxis='y1',line=dict(width=1,color='black'))\n",
    "               ,row=1,col=1)\n",
    "fig1.add_trace(go.Scatter(x=data[\"Timestamp\"], y=data[\"rising_cpboolean\"],name= \"hrv_inv\",yaxis='y2',line=dict(width=1,color='red'))\n",
    "               ,row=1,col=1,secondary_y=True)\n",
    "fig1.add_trace(go.Scatter(x=data[\"Timestamp\"], y=data[\"magGyro_driver_moving\"],name= \"Magnitude of Gyro\",yaxis='y3',line=dict(width=1,color='black'))\n",
    "               ,row=2,col=1)\n",
    "fig1.add_trace(go.Scatter(x=data[\"Timestamp\"], y=data[\"entropy\"],name= \"Gaze Entropy\",yaxis='y4',line=dict(width=1,color='red'))\n",
    "               ,row=2,col=1,secondary_y=True)\n",
    "fig1.add_trace(go.Scatter(x=data[\"Timestamp\"], y=data[\"all_vehicles_moving\"],name= \"No. of Vehicles\",yaxis='y5',line=dict(width=1,color='black'))\n",
    "               ,row=3,col=1)\n",
    "fig1.add_trace(go.Scatter(x=data[\"Timestamp\"], y=data[\"entropy\"],name= \"Intersection\",yaxis='y6',line=dict(width=1,color='red'))\n",
    "               ,row=3,col=1,secondary_y=True)\n",
    "# fig1.add_trace(go.Scatter(x=data[\"Timestamp\"], y=data[\"all_vehicles_bcp_probinv\"],name= \"HR\",yaxis='y1',line=dict(width=4,color='black'))\n",
    "#                ,row=6,col=1)\n",
    "# fig1.add_trace(go.Scatter(x=data[\"Timestamp\"], y=data[\"follow_dist_bcp_probinv\"],name= \"Vehicles\",yaxis='y1',line=dict(width=4,color='black'))\n",
    "#                ,row=7,col=1)\n",
    "# fig1.add_trace(go.Scatter(x=data[\"Timestamp\"], y=data[\"traffic_light_moving\"],name= \"Follow_dist\",yaxis='y1',line=dict(width=4,color='black'))\n",
    "#                ,row=8,col=1)\n",
    "\n",
    "\n",
    "\n",
    "fig1.update_layout(  \n",
    "    {    \n",
    "        'yaxis':{'title':\"<b>entropy\",'linecolor':'black'},\n",
    "        'yaxis2':{'title':\"<b>HRV - RMSSD\",'linecolor':'black'},\n",
    "        'yaxis3':{'title':\"<b>Magnitude of Gyro \",'linecolor':'black'},\n",
    "        'yaxis4':{'title':\"<b>Gaze Entropy\",'linecolor':'black'},\n",
    "        'yaxis5':{'title':\"<b>No. of Vehicles\",'linecolor':'black'},\n",
    "        'yaxis6':{'title':\"<b>Intersection\",'linecolor':'black'},\n",
    "        'yaxis7':{'title':\"<b>Follow_dist\",'linecolor':'black'},\n",
    "        'yaxis8':{'title':\"<b>Traffic-light-moving\",'linecolor':'black'},\n",
    "        'height':1080,\n",
    "        'width':1920,\n",
    "        'font':dict(size=20),\n",
    "        'paper_bgcolor':'rgba(0,0,0,0)',\n",
    "        'plot_bgcolor':'rgba(0,0,0,0)'\n",
    "        \n",
    "    }\n",
    ")\n",
    "fig1.update_layout(legend=dict(\n",
    "    yanchor=\"top\",\n",
    "    xanchor=\"left\",\n",
    "    \n",
    "))\n",
    "\n",
    "\n",
    "fig1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv(\"state_space_model_spaced_change_points.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Space change points in a different way with min+sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_for_peak = pd.read_csv(\"state_space_model_1.csv\",parse_dates=[\"Timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_peak[\"Timestamp\"] = data_for_peak[\"Timestamp\"].dt.tz_localize(None)\n",
    "data_for_peak[\"diff\"] = data_for_peak[\"HR_bcp_mean\"].diff()\n",
    "data_for_peak[\"diff\"] = data_for_peak[\"diff\"].shift(-1)\n",
    "data_for_peak[\"type_of_cp\"] = data_for_peak[\"diff\"].apply(lambda x: 1 if x>(0) else 0)\n",
    "data_for_peak[\"rising_cp\"] = 0\n",
    "data_for_peak[\"rising_cp\"][(data_for_peak[\"HR_bcp_prob\"]!=0)&(data_for_peak[\"type_of_cp\"]==1)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "\n",
    "x = data_for_peak[\"rising_cp\"]\n",
    "peaks, _ = find_peaks(x, height=1,distance=300)\n",
    "#sns.distplot(peaks[1:]-peaks[:-1])\n",
    "boolean_obs = np.zeros(x.shape[0])\n",
    "boolean_obs[peaks] = 1\n",
    "data_for_peak[\"rising_cp\"+\"boolean\"] = boolean_obs\n",
    "data_for_peak[\"rising_cp\"+\"coin_prob\"] = data_for_peak[\"rising_cp\"+\"boolean\"].rolling(window=240,min_periods=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def above_sd(segment):\n",
    "    if segment>(2*data_for_peak[\"HR_x\"].std()+data_for_peak[\"HR_x\"].mean()):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "data_for_peak[\"above_sd\"] = data_for_peak[\"HR_bcp_mean\"].apply(above_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_any_peak(segment):    \n",
    "    if ((segment > 0.1)).any():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "#data_all_speed_19[\"overlapping\"] = \n",
    "data_for_peak[\"above_sd_presence\"] = data_for_peak[\"above_sd\"].rolling(3).apply(find_any_peak).dropna()\n",
    "data_for_peak[\"rising_cp_presence\"] = data_for_peak[\"rising_cp\"].rolling(3).apply(find_any_peak).dropna()\n",
    "data_for_peak[\"abnormal_loc\"] =data_for_peak[\"above_sd_presence\"] + data_for_peak[\"rising_cp_presence\"]\n",
    "data_for_peak[\"abnormal_loc\"][data_for_peak[\"abnormal_loc\"]!=2] =0\n",
    "data_for_peak[\"abnormal_loc\"][data_for_peak[\"abnormal_loc\"]==2] =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12. plotting the smooth\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import wasserstein_distance\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import scipy\n",
    "from scipy import signal\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "#\n",
    "data = data_for_peak.copy()\n",
    "fig1 = make_subplots(rows=1, cols=1, \n",
    "                    shared_xaxes=True,vertical_spacing =0.05,specs=[[{\"secondary_y\": True}]\n",
    "                                                                   ])\n",
    "fig1.add_trace(go.Scatter(x=data[\"Timestamp\"], y=data[\"HR_bcp_mean\"],name= \"Heart Rate\",yaxis='y1',line=dict(width=1,color='black'))\n",
    "               ,row=1,col=1)\n",
    "fig1.add_trace(go.Scatter(x=data[\"Timestamp\"], y=data[\"abnormal_loc\"],name= \"Segments\",yaxis='y2',line=dict(width=1,color='red'))\n",
    "               ,row=1,col=1,secondary_y=True)\n",
    "# fig1.add_trace(go.Scatter(x=data[\"Timestamp\"], y=data[\"abnormal_loc\"],name= \"Change Points\",yaxis='y2',line=dict(width=1,color='red'))\n",
    "#                ,row=2,col=1)\n",
    "# fig1.add_trace(go.Scatter(x=data[\"Timestamp\"], y=data[\"above_sd\"],name= \"Change Points\",yaxis='y2',line=dict(width=1,color='red'))\n",
    "#                ,row=3,col=1)\n",
    "\n",
    "\n",
    "\n",
    "fig1.update_layout(  \n",
    "    {    \n",
    "        'xaxis':{'title':\"<b>Time\",'linecolor':'black'},\n",
    "        'yaxis':{'title':\"<b>HR\",'linecolor':'black'},\n",
    "        'yaxis2':{'title':\"<b>Change Points\",'linecolor':'black'},\n",
    "        'height':400,\n",
    "        'width':1200,\n",
    "        'font':dict(size=20),\n",
    "        'paper_bgcolor':'rgba(0,0,0,0)',\n",
    "        'plot_bgcolor':'rgba(0,0,0,0)'\n",
    "        \n",
    "    }\n",
    ")\n",
    "fig1.update_layout(legend=dict(\n",
    "    yanchor=\"top\",\n",
    "    xanchor=\"left\",\n",
    "    \n",
    "))\n",
    "\n",
    "\n",
    "fig1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_peak.to_csv(\"state_space_model_spaced_change_point_sd.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting segments together with their State-space results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_for_res = pd.read_csv(\"state_space_model_spaced_change_point_sd.csv\")\n",
    "results_of_state_space = pd.read_csv(\"change_point_based_state_space.csv\")\n",
    "results_of_state_space = results_of_state_space.dropna()\n",
    "for index,row in results_of_state_space.iterrows():\n",
    "    for col in results_of_state_space.columns:\n",
    "        data_for_res.loc[row[\"start\"]:row[\"end\"],col] =row[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12. plotting the smooth\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import wasserstein_distance\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import scipy\n",
    "from scipy import signal\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "#\n",
    "data = data_for_res.copy()\n",
    "fig1 = make_subplots(rows=2, cols=1, \n",
    "                    shared_xaxes=True,vertical_spacing =0.05,specs=[[{\"secondary_y\": True}],[{\"secondary_y\": True}]\n",
    "                                                                   ])\n",
    "fig1.add_trace(go.Scatter(x=data[\"Timestamp\"], y=data[\"HR_x\"],name= \"Heart Rate\",yaxis='y1',line=dict(width=1,color='black'))\n",
    "               ,row=1,col=1)\n",
    "fig1.add_trace(go.Scatter(x=data[\"Timestamp\"], y=data[\"abnormal_loc\"],name= \"Segments\",yaxis='y2',line=dict(width=1,dash=\"dash\",color='red'))\n",
    "               ,row=1,col=1,secondary_y=True)\n",
    "fig1.add_trace(go.Scatter(x=data[\"Timestamp\"], y=data[\"B.b1\"],name= \"Stress Transition\",yaxis='y3',line=dict(width=4,color='blue'))\n",
    "               ,row=2,col=1)\n",
    "fig1.add_trace(go.Scatter(x=data[\"Timestamp\"], y=data[\"B.b4\"],name= \"Work Load Transition\",yaxis='y4',line=dict(width=4,color='green'))\n",
    "               ,row=2,col=1,secondary_y=True)\n",
    "\n",
    "\n",
    "\n",
    "fig1.update_layout(  \n",
    "    {    \n",
    "        \n",
    "        'yaxis':{'title':\"<b>HR\",'linecolor':'black'},\n",
    "        'yaxis2':{'title':\"<b>Change Points\",'linecolor':'black'},\n",
    "        'yaxis3':{'title':\"<b>Transition Coefficient\",'linecolor':'black'},\n",
    "        'yaxis4':{'title':\"<b>Transition Coefficient\",'linecolor':'black'},\n",
    "        'xaxis2':{'title':\"<b>Time\",'linecolor':'black'},\n",
    "        'height':1100,\n",
    "        'width':1820,\n",
    "        'font':dict(size=20),\n",
    "        'paper_bgcolor':'rgba(0,0,0,0)',\n",
    "        'plot_bgcolor':'rgba(0,0,0,0)'\n",
    "        \n",
    "    }\n",
    ")\n",
    "fig1.update_layout(legend=dict(\n",
    "    yanchor=\"top\",\n",
    "    xanchor=\"left\",\n",
    "    orientation=\"h\",\n",
    "    x=0,\n",
    "    y=1.09\n",
    "    \n",
    "))\n",
    "\n",
    "\n",
    "fig1.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the segmented state space results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_state_space = pd.read_csv(\"results_state_space_10.csv\")\n",
    "results_state_space[\"file_number\"] = results_state_space[\"name\"].str.split(\"_\",n=1).str[0]\n",
    "results_state_space[\"file_number\"] = pd.to_numeric(results_state_space[\"file_number\"])\n",
    "results_state_space = results_state_space.sort_values(by=[\"file_number\"])\n",
    "results_state_space = results_state_space[results_state_space['errors']==0]\n",
    "\n",
    "\n",
    "data_for_res = pd.read_csv(\"state_space_model_spaced_change_point_sd.csv\",parse_dates=[\"Timestamp\"])\n",
    "for index,row in results_state_space.iterrows():\n",
    "    for col in results_state_space.columns:\n",
    "        data_for_res.loc[row[\"file_number\"]:row[\"file_number\"]+10,col] =row[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_res['cross_corr'] = data_for_res['B.b1'].rolling(400,min_periods=0).corr(data_for_res['B.b4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_res = data_for_res.dropna(subset=['file_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "def crosscorr(datax, datay, lag=0, wrap=False):\n",
    "    \"\"\" Lag-N cross correlation. \n",
    "    Shifted data filled with NaNs \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lag : int, default 0\n",
    "    datax, datay : pandas.Series objects of equal length\n",
    "    Returns\n",
    "    ----------\n",
    "    crosscorr : float\n",
    "    \"\"\"\n",
    "    if wrap:\n",
    "        shiftedy = datay.shift(lag)\n",
    "        shiftedy.iloc[:lag] = datay.iloc[-lag:].values\n",
    "        return datax.corr(shiftedy)\n",
    "    else: \n",
    "        return datax.corr(datay.shift(lag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def time_lagged_cross_corr(sigA,sigB,window_size=120,step_size=1):\n",
    "    t_start = 0\n",
    "    t_end = t_start + window_size\n",
    "    rss=[]\n",
    "    while t_end < sigA.shape[0]:\n",
    "        d1 = sigA.iloc[t_start:t_end]\n",
    "        d2 = sigB.iloc[t_start:t_end]\n",
    "        rs = [crosscorr(d1,d2, lag, wrap=False) for lag in range(-200,210,10)]\n",
    "        rss.append(rs)\n",
    "        t_start = t_start + step_size\n",
    "        t_end = t_end + step_size\n",
    "    rss = pd.DataFrame(rss,columns=range(-200,210,10))\n",
    "    \n",
    "    return rss.idxmax(axis=1)\n",
    "\n",
    "data_for_res['lag_val'] = 0\n",
    "data_for_res['lag_val'][240:] = time_lagged_cross_corr(data_for_res['B.b1'],data_for_res['B.b4'],240,1)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crosscorr(datax, datay, window_size, lag=0):\n",
    "    \"\"\" Lag-N cross correlation. \n",
    "    Parameters\n",
    "    ----------\n",
    "    lag : int, default 0\n",
    "    datax, datay : pandas.Series objects of equal length\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    crosscorr : float\n",
    "    \"\"\"\n",
    "    return datax.rolling(window_size, center=True).corr(datay.shift(lag))\n",
    "\n",
    "\n",
    "WCC = np.array([crosscorr(data_for_res['B.b1'], data_for_res['B.b4'], window_size=120, lag=i) for i in range(-200,200+1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import argrelextrema\n",
    "\n",
    "def WCC_lag(WCC):\n",
    "    center = int(np.floor(WCC.shape[0]/2))\n",
    "    right = WCC[center:,]\n",
    "    left = WCC[:center,]\n",
    "    lag_lists = []\n",
    "    for i in range(right.shape[1]):\n",
    "        right_temp = right[:,i]\n",
    "        #print(right_temp)\n",
    "        left_temp = left[:,i]\n",
    "        \n",
    "        try:\n",
    "            lag_right = argrelextrema(right_temp, np.greater)[0][0]\n",
    "        except:\n",
    "            try:\n",
    "                if (np.argmax(right_temp)==right_temp.shape[0]-1) or (np.argmax(right_temp)==0):\n",
    "                    lag_right = np.argmax(right_temp)\n",
    "            except:\n",
    "                lag_right = np.inf\n",
    "        try:\n",
    "            lag_left = argrelextrema(left_temp, np.greater)[0][-1] - center\n",
    "        except:\n",
    "            try:\n",
    "                if (np.argmax(left_temp)==left_temp.shape[0]-1) or (np.argmax(left_temp)==0):\n",
    "                    lag_left = np.argmax(left_temp)\n",
    "            except:\n",
    "                lag_left = np.inf\n",
    "        lags = np.array([lag_left,lag_right])\n",
    "        #print(lags)\n",
    "        min_lag = lags[np.argmin(abs(lags))]#min(abs(lag_right),abs(lag_left))\n",
    "        lag_lists.append(min_lag)\n",
    "    return lag_lists\n",
    "WCC_output = WCC_lag(WCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WCC_output = pd.DataFrame(np.array(WCC_output))\n",
    "data_for_res['lag_val'] = WCC_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring and clean speed data\n",
    "\n",
    "speed_0305 = pd.read_csv(\"I:/Phase 1/9/Video/03072021/speed_csv/final_speed.csv\")\n",
    "speed_0305[\"Timestamp\"] = speed_0305['Date'].str.cat(speed_0305['Time'],sep=\"T\")\n",
    "speed_0305[\"Timestamp\"] = pd.to_datetime(speed_0305[\"Timestamp\"], format=\"%m/%d/%YT%H:%M:%S\")\n",
    "speed_0305 = speed_0305.sort_values(by=\"Timestamp\")\n",
    "speed_0305[\"Timestamp\"] = speed_0305[\"Timestamp\"].dt.tz_localize(None)\n",
    "speed_0305[\"Normalized Speed\"] = speed_0305[\"Normalized Speed\"].replace(to_replace=\"not available\",method='bfill') \n",
    "speed_0305[\"Timestamp\"] = speed_0305[\"Timestamp\"] + pd.Timedelta(value=+4,unit='h')\n",
    "start_date = pd.Timestamp(\"2021-03-05T20:35:20.000+0000\",tz=None)\n",
    "end_date = pd.Timestamp(\"2021-03-05T23:26:54.000+0000\",tz=None)\n",
    "start_date = start_date.tz_localize(None)\n",
    "end_date = end_date.tz_localize(None)\n",
    "mask = (speed_0305['Timestamp'] > start_date) & (speed_0305['Timestamp'] <= end_date)\n",
    "speed_0305 = speed_0305.loc[mask]\n",
    "#speed_for_bcp = total_data.resample('1S').mean().bfill().ffill()\n",
    "#speed_for_bcp = data_for_bcp.reset_index()\n",
    "\n",
    "speed_0305.to_csv(\"cleaned_speed_0305_for_bcp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform bcp and bring the file back\n",
    "\n",
    "speed_0305_bcp = pd.read_csv(\"D:/Google Drive/UVA_PhD/Projects/Structural Equation Modeling/cleaned_speed_0305_added_bcp.csv\",parse_dates=[\"Timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retreive max lag based steve's method\n",
    "\n",
    "lags_steve = pd.read_csv(\"D:/Google Drive/UVA_PhD/Projects/Structural Equation Modeling/codeforwcc/results.csv\")\n",
    "data_for_res[\"lag_val_steve\"]=np.nan\n",
    "data_for_res['lag_val_steve'][-7822:] =lags_steve[\"V1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12. plotting the smooth\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import wasserstein_distance\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import scipy\n",
    "from scipy import signal\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "#\n",
    "data = data_for_res.copy()\n",
    "fig1 = make_subplots(rows=3, cols=1, \n",
    "                    shared_xaxes=True,vertical_spacing =0.05,specs=[[{\"secondary_y\": True}],[{\"secondary_y\": True}],\n",
    "                                                                    [{\"secondary_y\": True}]\n",
    "                                                                   ])\n",
    "fig1.add_trace(go.Scatter(x=data[\"Timestamp\"], y=data[\"B.b1\"],name= \"Stress Transition\",yaxis='y1',line=dict(width=1,color='black'))\n",
    "               ,row=1,col=1)\n",
    "fig1.add_trace(go.Scatter(x=data[\"Timestamp\"], y=data[\"B.b4\"],name= \"Work Load Transition\",yaxis='y2',line=dict(width=1,dash=\"dash\",color='red'))\n",
    "               ,row=1,col=1,secondary_y=True)\n",
    "fig1.add_trace(go.Scatter(x=data[\"Timestamp\"], y=data[\"cross_corr\"],name= \"Cross Correlation\",yaxis='y3',mode='markers')\n",
    "               ,row=2,col=1)\n",
    "fig1.add_trace(go.Scatter(x=data[\"Timestamp\"], y=data[\"lag_val_steve\"],name= \"Highest Lag\",yaxis='y4',mode='markers')\n",
    "               ,row=2,col=1,secondary_y=True)\n",
    "fig1.add_trace(go.Scatter(x=speed_0305_bcp[\"Timestamp\"], y=speed_0305_bcp[\"Normalized.Speed\"],name= \"Speed\",yaxis='y4')\n",
    "               ,row=3,col=1)\n",
    "fig1.add_trace(go.Scatter(x=speed_0305_bcp[\"Timestamp\"], y=speed_0305_bcp[\"bcp_prob_speed\"],name= \"BCP Speed\",yaxis='y4')\n",
    "               ,row=3,col=1,secondary_y=True)\n",
    "\n",
    "fig1.update_layout(  \n",
    "    {    \n",
    "        \n",
    "        'yaxis1':{'title':\"<b>Stress Transition\",'linecolor':'black'},\n",
    "        'yaxis2':{'title':\"<b>Work Load Transition\",'linecolor':'black'},\n",
    "        'yaxis3':{'title':\"<b>Cross Correlation\",'linecolor':'black'},\n",
    "        'yaxis4':{'title':\"<b>Highest Lag\",'linecolor':'black'},\n",
    "        'yaxis5':{'title':\"<b>Speed\",'linecolor':'black'},\n",
    "        'yaxis6':{'title':\"<b>BCP Speed\",'linecolor':'black'},\n",
    "        'xaxis3':{'title':\"<b>Time\",'linecolor':'black'},\n",
    "        'height':1300,\n",
    "        'width':1000,\n",
    "        'font':dict(size=20),\n",
    "        'paper_bgcolor':'rgba(0,0,0,0)',\n",
    "        'plot_bgcolor':'rgba(0,0,0,0)'\n",
    "        \n",
    "    }\n",
    " )\n",
    "fig1.update_layout(legend=dict(\n",
    "    yanchor=\"top\",\n",
    "    xanchor=\"left\",\n",
    "    orientation=\"h\",\n",
    "    x=0,\n",
    "    y=1.09\n",
    "    \n",
    "))\n",
    "\n",
    "\n",
    "fig1.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting for the purpose of Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "data = final_data.copy()\n",
    "list_of_feats=[\"Timestamp\",\"HRV_RMSSD\",\"entropy\",\"all_vehicles_moving\",\"magGyro_driver_moving\",\"traffic_light_moving\"]\n",
    "data=data[list_of_feats]\n",
    "data=data.set_index([\"Timestamp\"])\n",
    "# create a scaler object\n",
    "scaler = MinMaxScaler()\n",
    "# fit and transform the data\n",
    "df_norm = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n",
    "df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import wasserstein_distance\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import scipy\n",
    "from scipy import signal\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "#\n",
    "fig1 = make_subplots(rows=1, cols=1, \n",
    "                    shared_xaxes=True,vertical_spacing =0.05)\n",
    "fig1.add_trace(go.Scatter(x=data.index, y=df_norm[\"HRV_RMSSD\"],name= \"RMSSD\",yaxis='y1',line=dict(width=1,color='black'))\n",
    "               ,row=1,col=1)\n",
    "fig1.add_trace(go.Scatter(x=data.index, y=df_norm[\"entropy\"],name= \"Gaze Entropy\",yaxis='y1',line=dict(width=1,color='red'))\n",
    "               ,row=1,col=1)\n",
    "fig1.add_trace(go.Scatter(x=data.index, y=df_norm[\"all_vehicles_moving\"],name= \"No. of Vehicles\",yaxis='y1',line=dict(width=1,color='blue'))\n",
    "               ,row=1,col=1)\n",
    "fig1.add_trace(go.Scatter(x=data.index, y=df_norm[\"magGyro_driver_moving\"],name= \"Hand Movement\",yaxis='y1',line=dict(width=1,color='orange'))\n",
    "               ,row=1,col=1)\n",
    "fig1.add_trace(go.Scatter(x=data.index, y=df_norm[\"traffic_light_moving\"],name= \"No. of Traffic Lights\",yaxis='y1',line=dict(width=1,color='green'))\n",
    "               ,row=1,col=1)\n",
    "\n",
    "\n",
    "\n",
    "fig1.update_layout(  \n",
    "    {   \n",
    "        'xaxis':{'title':\"<b>Time\",'linecolor':'black'},\n",
    "        'yaxis':{'title':\"<b>Normalized Variable\",'linecolor':'black'},\n",
    "        'height':700,\n",
    "        'width':1920,\n",
    "        'font':dict(size=20),\n",
    "        'paper_bgcolor':'rgba(0,0,0,0)',\n",
    "        'plot_bgcolor':'rgba(0,0,0,0)'\n",
    "        \n",
    "    }\n",
    ")\n",
    "fig1.update_layout(legend=dict(\n",
    "    yanchor=\"top\",\n",
    "    xanchor=\"left\",\n",
    "    \n",
    "))\n",
    "\n",
    "\n",
    "fig1.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For visualizing peaks you can run the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import plotly.tools as tls\n",
    "from scipy.signal import find_peaks,detrend\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dat = data_for_peak.copy()\n",
    "x = dat[\"all_vehicle_bcp_prob\"]\n",
    "peaks, _ = find_peaks(x, height=1)\n",
    "boolean_obs = np.zeros(x.shape[0])\n",
    "boolean_obs[peaks] = 1\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "#df_new['phasic'].plot()a\n",
    "ax.plot(x.values)\n",
    "ax.plot(peaks, x[peaks], \"x\",color=\"gray\")\n",
    "ax.plot(np.zeros_like(x), \"--\", color=\"gray\")\n",
    "#plt.ylim([8.7,11])\n",
    "dat['boolean'] = boolean_obs\n",
    "#tls.mpl_to_plotly(fig)\n",
    "dat['coinprob'] = dat['boolean'].rolling(window=240).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
